{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the Big Brain challenge\n",
    "\n",
    "Today we will be attempting to make the rules and structure of a human brain.\n",
    "\n",
    "We will use 3D matricies to represent the 3D structure of a real brain.\n",
    "\n",
    "You must solve the following issues:\n",
    "1. passing a spike from one neuron to the next.\n",
    "2. passing spikes in all connection directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_1D_2D_brain(data, title):\n",
    "    # plot as an image\n",
    "    if data.dim() == 1:\n",
    "        # Reshape the tensor to a 2D array with shape (1, len(data))\n",
    "        data = data.unsqueeze(0)\n",
    "\n",
    "    # convert to numpy\n",
    "    data = data.numpy()\n",
    "    # Remove axis ticks\n",
    "    plt.xticks(ticks=range(data.shape[1]), labels=range(data.shape[1]))\n",
    "    plt.yticks(ticks=range(data.shape[0]), labels=range(data.shape[0]))\n",
    "\n",
    "    plt.imshow(data, cmap='gray', aspect='equal')\n",
    "\n",
    "    # put the value of each pixel in the image\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            plt.text(j, i, f'{data[i, j]:.2f}', color='red', ha='center', va='center')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE 1D SCENARIO: \n",
    "We can shift spikes left and right or not at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_state(state, direction):\n",
    "    dir_right = direction == 1  # true where there is +1, else False\n",
    "    dir_left = direction == -1  # true where there is -1, else False\n",
    "\n",
    "    state_right = state*dir_right\n",
    "    state_left = state*dir_left\n",
    "\n",
    "    # shift state_right right by 1\n",
    "    state_right_new = torch.roll(state_right, shifts=1)\n",
    "\n",
    "    # shift state_right left by 1\n",
    "    state_left_new = torch.roll(state_left, shifts=-1)\n",
    "\n",
    "    # sum them together\n",
    "    new_state = state_left_new + state_right_new\n",
    "\n",
    "    return new_state\n",
    "\n",
    "\n",
    "state = torch.tensor([1, 0, 0, 1, 0, 1], dtype=torch.bool)\n",
    "connections  = torch.tensor([1, 0, 1, -1, 1, -1], dtype=torch.int)\n",
    "# plot_1D_2D_brain(connections, 'connections')\n",
    "\n",
    "states = []\n",
    "for time_step in range(10):\n",
    "    state = move_state(state, connections)\n",
    "    states.append(state)\n",
    "\n",
    "states = torch.stack(states)\n",
    "# plot_1D_2D_brain(states, '1D Brain at different time steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE 2D SCENARIO:\n",
    "I can and I will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAE4CAYAAAAHCboIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAffklEQVR4nO3dfXCU1fnG8WuBEGIIKQEJRMKbCoK8WIhCqAaoEqSCoiLVWg0KihQQZCiC1ClYBlBGio7Ar1pFSwehLQKORQSnvElACRVBgorylioE44ybgBAInN8fCZFNsgmB232yyfczcybu2ft5nrN3VnO5uyfxOeecAAAADNTyegEAAKD6IFgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAHkhPT9fUqVP1/fffl7qvd+/e6t27d8jXdClWrVqlqVOner2MMuXl5enxxx/XFVdcocjISLVt21bPPfeczpw5U6r2o48+Ur9+/RQTE6P69eurT58+2rx5swerBsIXwQLwQHp6uqZNm1ZmsJg/f77mz58f+kVdglWrVmnatGleL6OUgoIC9e3bV3//+9/11FNP6Z133tHAgQM1adIkPfHEEwG127ZtU0pKik6cOKFFixZp0aJFOnnypG6++WZt2bLFo0cAhJ86Xi8AQKAOHTp4vYRq41//+pc+/PBDLVu2THfddZckqW/fvjp27JjmzZunUaNGqV27dpKkp59+Wj/72c+0evVqXXbZZZKkW265RW3atNGECRN45QK4QLxiAYTY1KlT9fvf/16S1Lp1a/l8Pvl8Pq1fv15S6bdCDhw4IJ/Pp9mzZ+vZZ59Vq1atFBUVpd69e+uLL77Q6dOnNWnSJCUkJCg2NlZ33nmnjh49Wuq6S5cuVXJysqKjo1W/fn3169dPH3/8cYXr/eGHHzRhwgS1bt1a9erVU1xcnJKSkvTmm29KkoYOHap58+ZJUvFj8fl8OnDggCTJOaf58+fruuuuU1RUlBo2bKjBgwdr3759Adfp3bu3OnbsqE2bNqlHjx6KiorSFVdcoaeffrrMty0uxObNm+Xz+dS/f/+A+QEDBujs2bNavnx5QG3v3r2LQ4UkxcTEKCUlRenp6Tp8+PBFrQGoaQgWQIgNHz5cY8aMkSS99dZb2rJli7Zs2aKuXbuWe9y8efO0efNmzZs3T3/961/12WefaeDAgRo2bJi+/fZbvfbaa3ruuef0/vvva/jw4QHHzpgxQ/fdd586dOigf/zjH1q0aJHy8vJ00003KTMzs9zrjh8/XgsWLNDjjz+u1atXa9GiRbrnnnv03XffSSr8P/3BgwdLUvFj2bJli5o1ayZJGjFihMaNG6dbbrlFK1as0Pz587V792717NlT2dnZAdc6cuSI7r33Xt1///1auXKlBg8erOnTp2vs2LEBdUOHDg0IL8GcOnVKtWrVUkRERMB8ZGSkJGnnzp0Btefmy6rdtWtXudcCUMQBCLnZs2c7SW7//v2l7uvVq5fr1atX8e39+/c7Sa5Lly7uzJkzxfNz5851ktztt98ecPy4ceOcJOf3+51zzh06dMjVqVPHjRkzJqAuLy/PNW3a1A0ZMqTctXbs2NENGjSo3JpRo0a5sv5zsmXLFifJPf/88wHzWVlZLioqyk2cODHgcUtyK1euDKh95JFHXK1atdzBgweL5x5++GFXu3Ztd+DAgXLXda5HmzZtCph/+umnnSSXmppaPHfddde5tm3bBvT49OnTrk2bNk6SW7x4cbnXAlCIVyyAMPGrX/1KtWr9+K9s+/btJUm33XZbQN25+UOHDkmS3nvvPRUUFOjBBx9UQUFB8ahXr5569epV/BZMMDfccIPeffddTZo0SevXr9eJEycueM3vvPOOfD6ffvvb3wZcu2nTpurSpUupa8fExOj2228PmPvNb36js2fPauPGjcVzr776qgoKCtSyZctyr3///fcrLi5Ojz76qD788EN9//33evPNN/Xiiy9KUkA/x4wZoy+++EKjR4/W119/raysLD322GM6ePBgqVoAwfFvChAm4uLiAm7XrVu33PmTJ09KUvHbDddff70iIiICxtKlS5WTk1PudV988UU9+eSTWrFihfr06aO4uDgNGjRIe/furXDN2dnZcs4pPj6+1LW3bt1a6trx8fGlztG0aVNJKn7rpTIaN26s1atXS5J69Oihhg0basyYMZozZ44k6YorriiuffjhhzVr1iwtWrRIzZs3V4sWLZSZmakJEyaUqgUQHLtCgGqucePGkgp3SFT0f/hliY6O1rRp0zRt2jRlZ2cXv3oxcOBAffbZZxVe2+fzadOmTeV+fuGckp+5kAo/dyFJjRo1qvTapcJAlZmZqQMHDuj48eO6+uqrtX37dklSSkpKQO2TTz6pcePGae/evYqJiVHLli01YsQIRUdHq1u3bhd1faCmIVgAHjj3A7UybytcrH79+qlOnTr66quvdPfdd1/SueLj4zV06FB98sknmjt3rn744QdddtllAY8nKiqquH7AgAGaNWuWvv76aw0ZMqTC8+fl5entt98OeDtk8eLFqlWrVqkQUFmtWrWSVLhL5fnnn1dCQoLuueeeUnWRkZHq2LGjpMK3k5YuXapHHnkk4HEBCI5gAXigU6dOkqQXXnhBaWlpioiIULt27RQTE2N+rVatWumZZ57RlClTtG/fPt16661q2LChsrOz9dFHHxW/IhFM9+7dNWDAAHXu3FkNGzbUnj17tGjRIiUnJxdvzTz3eJ599ln1799ftWvXVufOnfWLX/xCjz76qB566CFlZGQoJSVF0dHROnz4sD744AN16tRJI0eOLL5Wo0aNNHLkSB06dEht27bVqlWr9Morr2jkyJFq0aJFcd2wYcP0xhtv6KuvvqrwVZgpU6aoU6dOatasmQ4dOqTXXntNH374of79738HhIVPP/1Uy5YtU1JSkiIjI/XJJ59o1qxZuvrqq/WnP/3ponoP1Ehef3oUqKkmT57sEhISXK1atZwkt27dOudc8F0hs2fPDjh+3bp1TpL75z//GTC/cOFCJ8lt27YtYH7FihWuT58+rkGDBi4yMtK1bNnSDR482L3//vvlrnPSpEkuKSnJNWzY0EVGRro2bdq4J554wuXk5BTX5Ofnu+HDh7vLL7/c+Xy+UjteXnvtNde9e3cXHR3toqKi3JVXXukefPBBl5GRUVzTq1cvd+2117r169e7pKQkFxkZ6Zo1a+aeeuopd/r06YA1paWlBd1VU9LIkSNdixYtXN26dV3jxo3d3Xff7Xbu3Fmq7vPPP3cpKSkuLi7O1a1b11111VXuD3/4gzt27FiF1wDwI59zznkZbABAKvwFWTk5Ofr000+9XgqAS8CuEAAAYIZgAQAAzPBWCAAAMMMrFgAAwAzBAgAAmCFYAAAAMyH/BVlnz57VN998o5iYGPl8vlBfHgAAXATnnPLy8pSQkFDuH+ULebD45ptvlJiYGOrLAgAAA1lZWWrevHnQ+0P+VshP8SuLAQBAaFT0czzkwYK3PwAACF8V/Rznw5sAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMBMjQgWIyXtk3RCUoakGyuoTymqOyHpK0kjyqi5S9JuSSeLvg4yWmtVRy/t0Esb9NEOvbRTo3vpLsK8efNcq1atXGRkpOvatavbuHHjBR/r9/udpJCNIZLLl9wwyV0juT9LLk9yiUHqW0nuWFHdNUXH5UvurvNqekjutOQmSa5d0ddTkrshhI/Li0Ev6WVVG/SRXlbFUd176ff7y/05X+lgsWTJEhcREeFeeeUVl5mZ6caOHeuio6PdwYMHq2Sw2Cq5+SXmMiU3I0j9rKL7z59bILn0824vkdyqEjXvSm6xB99gehmeg17Sx6o26CW9vNBRUbCo9Fshc+bM0bBhwzR8+HC1b99ec+fOVWJiohYsWFDZU/3kIiR1k7SmxPwaST2DHJNcRv17kpIk1amgJtg5qwN6aYde2qCPduilHXpZyc9YnDp1Stu3b1dqamrAfGpqqtLT000XZqGxCr8p2SXmsyU1DXJM0yD1EUXnK68m2DmrA3pph17aoI926KUdevljGLogOTk5OnPmjOLj4wPm4+PjdeTIkTKPyc/PV35+fvHt3Nzci1jmpXElbvvKmKuovuR8Zc9ZXdBLO/TSBn20Qy/t1OReXtSuEJ/PF3DbOVdq7pyZM2cqNja2eCQmJl7MJS9KjqQClU50TVQ6+Z1zJEj9aUnfVVAT7JzVAb20Qy9t0Ec79NIOvaxksGjcuLFq165d6tWJo0ePlnoV45zJkyfL7/cXj6ysrItfbSWdlrRdUt8S830lBXvjZksZ9akq3AZUUEFN1XszyA69tEMvbdBHO/TSDr2UKr0r5IYbbnAjR44MmGvfvr2bNGlSldwVcm7bz0Mq3MYzR4XbfloU3T9Dcm+cV99Khdt+ni+qf0ilt/0kq3Dbz0QVbvuZqJq1hYpe0suqMugjvayKo7r38ifbbvrqq6+6zMxMN27cOBcdHe0OHDhQJYOFJDdScvsld1JyGZK76bz7FkpuXYn6FMltL6rfJ7kRZZzzbsntKfrmZ0ruTg++uV4Mekkvq9qgj/SyKo7q3EvzYOFc4S/Iatmypatbt67r2rWr27BhwwUf60WwYDAYDAaDYTMqChY+55xTCOXm5io2NjaUlwQAAEb8fr8aNGgQ9P4a8bdCAABAaBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAmTpeLwBA9eKc83oJ1YLP5/N6CcBF4RULAABghmABAADMECwAAIAZggUAADBDsAAAAGYIFgAAwAzBAgAAmCFYAAAAMwQLAABghmABAADMECwAAIAZggUAADBDsAAAAGYIFgAAwAzBAgAAmCFYAAAAMwQLAABghmABAADMECwAAIAZggUAADBDsAAAAGYIFgAAwAzBAgAAmCFYAAAAMwQLAABghmABAADMECwAAIAZggUAADBDsAAAAGYIFgAAwAzBAgAAmCFYAAAAMwQLAABghmABAADMECwAAICZGhEsRkraJ+mEpAxJN1ZQn1JUd0LSV5JGlFFzl6Tdkk4WfR1ktNaqjl7aoZcGNm6UBg6UEhIkn09asaLiYzZskLp1k+rVk9q0kf7v/0rXLFsmdeggRUYWfl2+3HzpVRHPSTs1upcuxPx+v5MUsjFEcvmSGya5ayT3Z8nlSS4xSH0ryR0rqrum6Lh8yd11Xk0PyZ2W3CTJtSv6ekpyN4TwcXkx6CW9vJARUqtWOTdlinPLljknObd8efn1+/Y5d9llzo0d61xmpnOvvOJcRIRz//rXjzXp6c7Vru3cjBnO7dlT+LVOHee2bv0pH0kpPCfDd1T3Xvr9/vKfu5V9sm/YsMENGDDANWvWzElyyyv6F7mEUAeLrZKbX2IuU3IzgtTPKrr//LkFkks/7/YSya0qUfOu5BZ78A2ml+E5qnMvPaMLCBYTJzp3zTWBcyNGONejx4+3hwxx7tZbA2v69XPu3ntNlnmheE6G76juvawoWFT6rZDjx4+rS5cueumllyp7aMhFSOomaU2J+TWSegY5JrmM+vckJUmqU0FNsHNWB/TSDr300JYtUmpq4Fy/flJGhnT6dPk16emhWaMHeE7aoZc/rvmC9e/fX/379/8p1mKusQofYHaJ+WxJTYMc0zRIfUTR+Y6UUxPsnNUBvbRDLz105IgUHx84Fx8vFRRIOTlSs2bBa44cCd06Q4znpB16eRHBorLy8/OVn59ffDs3N/envmQprsRtXxlzFdWXnK/sOasLemmHXnrE5wu87Vzp+bJqSs5VQzwn7dTkXv7ku0Jmzpyp2NjY4pGYmPhTX7JYjqQClU50TVQ6+Z1zLhmWrD8t6bsKaoKdszqgl3bopYeaNi39ysPRo1KdOlKjRuXXlHwVoxrhOWmHXoYgWEyePFl+v794ZGVl/dSXLHZa0nZJfUvM95UU7N3SLWXUp6pwG1BBBTXV9x1YemmJXnooOVlauzZwbs0aKSlJiogov6ZnVXw32wbPSTv0Urqkj3BLVX9XyLltPw+pcBvPHBVu+2lRdP8Myb1xXn0rFW77eb6o/iGV3vaTrMJtPxNVuO1nomrWFip6SS/LGyGVl+fcxx8XDsm5OXMK//ngwcL7J01y7oEHfqw/t930iScKt5u++mrp7aabNxduN501q3C76axZNWq7aXV8TtJL22G+3bTkE7+qBwtJbqTk9kvupOQyJHfTefctlNy6EvUpktteVL9PciPKOOfdkttT9M3PlNydHnxzvRj0kl5WNEJq3brCQFFypKUV3p+W5lyvXoHHrF/v3M9/7lzdus61auXcggWlz/vPfzrXrl1h6LjmmsLfkxFiPCfDe1TnXlYULHxFT+ALduzYMX355ZeSpJ///OeaM2eO+vTpo7i4OLVo0aLC43NzcxUbG1uZSwIII5X8TwqC8NWAD4siPPn9fjVo0CDo/ZUOFuvXr1efPn1Kzaelpen111+v8HiCBVC9ESxsECxQVZkHi0tFsACqN4KFDYIFqqqKgkWN+CNkAAAgNAgWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADATB2vFwCgevH5fF4vAYCHeMUCAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCmRgSLkZL2STohKUPSjRXUpxTVnZD0laQRZdTcJWm3pJNFXwcZrbWqo5d26KUN+miHXtqp0b10Ieb3+52kkI0hksuX3DDJXSO5P0suT3KJQepbSe5YUd01RcflS+6u82p6SO605CZJrl3R11OSuyGEj8uLQS/pZVUb9JFeVsVR3Xvp9/vL/TlfqWAxY8YMl5SU5OrXr+8uv/xyd8cdd7jPPvusSgeLrZKbX2IuU3IzgtTPKrr//LkFkks/7/YSya0qUfOu5BZ78A2ml+E56CV9rGqDXtLLCx0VBYtKvRWyYcMGjRo1Slu3btXatWtVUFCg1NRUHT9+vDKnCZkISd0krSkxv0ZSzyDHJJdR/56kJEl1KqgJds7qgF7aoZc26KMdemmHXv645guyevXqgNsLFy5UkyZNtH37dqWkpJguzEJjFT7A7BLz2ZKaBjmmaZD6iKLzHSmnJtg5qwN6aYde2qCPduilHXpZyWBRkt/vlyTFxcUFrcnPz1d+fn7x7dzc3Eu55EVxJW77ypirqL7kfGXPWV3QSzv00gZ9tEMv7dTkXl70rhDnnMaPH68bb7xRHTt2DFo3c+ZMxcbGFo/ExMSLvWSl5UgqUOlE10Slk98555JhyfrTkr6roCbYOasDemmHXtqgj3bopR16eQnBYvTo0dq5c6fefPPNcusmT54sv99fPLKysi72kpV2WtJ2SX1LzPeVlB7kmC1l1KeqcBtQQQU1wc5ZHdBLO/TSBn20Qy/t0Evporabjh492jVv3tzt27ev0sd6td30IRVu45mjwm0/LYrunyG5N86rb6XCbT/PF9U/pNLbfpJVuO1nogq3/UxUzdpCRS/pZVUZ9JFeVsVR3Xtput307NmzbtSoUS4hIcF98cUXlQ4VXgQLSW6k5PZL7qTkMiR303n3LZTcuhL1KZLbXlS/T3Ijyjjn3ZLbU/TNz5TcnR58c70Y9JJeVrVBH+llVRzVuZcVBQufc87pAv3ud7/T4sWLtXLlSrVr1654PjY2VlFRURd0jtzcXMXGxl7oJQEAQBXi9/vVoEGDoPdXKlj4fL4y5xcuXKihQ4de0DkIFgAAhK+KgkWltptWIoMAAIAaqEb8ETIAABAaBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzBAsAAGCGYAEAAMwQLAAAgBmCBQAAMEOwAAAAZggWAADADMECAACYIVgAAAAzIQ8WzrlQXxIAABip6Od4yINFXl5eqC8JAACMVPRz3OdC/BLC2bNn9c033ygmJkY+ny+Ul75gubm5SkxMVFZWlho0aOD1csIavbRBH+3QSzv00ka49NE5p7y8PCUkJKhWreCvS9QJ4ZokSbVq1VLz5s1DfdmL0qBBgyr9TQ4n9NIGfbRDL+3QSxvh0MfY2NgKa/jwJgAAMEOwAAAAZggWZYiMjNQf//hHRUZGer2UsEcvbdBHO/TSDr20Ud36GPIPbwIAgOqLVywAAIAZggUAADBDsAAAAGYIFgAAwAzBooT58+erdevWqlevnrp166ZNmzZ5vaSwtHHjRg0cOFAJCQny+XxasWKF10sKSzNnztT111+vmJgYNWnSRIMGDdLnn3/u9bLC0oIFC9S5c+fiX0KUnJysd9991+tlhb2ZM2fK5/Np3LhxXi8l7EydOlU+ny9gNG3a1OtlXTKCxXmWLl2qcePGacqUKfr444910003qX///jp06JDXSws7x48fV5cuXfTSSy95vZSwtmHDBo0aNUpbt27V2rVrVVBQoNTUVB0/ftzrpYWd5s2ba9asWcrIyFBGRoZ++ctf6o477tDu3bu9XlrY2rZtm15++WV17tzZ66WErWuvvVaHDx8uHrt27fJ6SZeM7abn6d69u7p27aoFCxYUz7Vv316DBg3SzJkzPVxZePP5fFq+fLkGDRrk9VLC3rfffqsmTZpow4YNSklJ8Xo5YS8uLk6zZ8/WsGHDvF5K2Dl27Ji6du2q+fPna/r06bruuus0d+5cr5cVVqZOnaoVK1Zox44dXi/FFK9YFDl16pS2b9+u1NTUgPnU1FSlp6d7tCogkN/vl1T4AxEX78yZM1qyZImOHz+u5ORkr5cTlkaNGqXbbrtNt9xyi9dLCWt79+5VQkKCWrdurXvvvVf79u3zekmXLOR/hKyqysnJ0ZkzZxQfHx8wHx8fryNHjni0KuBHzjmNHz9eN954ozp27Oj1csLSrl27lJycrJMnT6p+/fpavny5OnTo4PWyws6SJUv03//+V9u2bfN6KWGte/fu+tvf/qa2bdsqOztb06dPV8+ePbV79241atTI6+VdNIJFCSX/lLtzrsr+eXfULKNHj9bOnTv1wQcfeL2UsNWuXTvt2LFD33//vZYtW6a0tDRt2LCBcFEJWVlZGjt2rNasWaN69ep5vZyw1r9//+J/7tSpk5KTk3XllVfqjTfe0Pjx4z1c2aUhWBRp3LixateuXerViaNHj5Z6FQMItTFjxujtt9/Wxo0b1bx5c6+XE7bq1q2rq666SpKUlJSkbdu26YUXXtBf/vIXj1cWPrZv366jR4+qW7duxXNnzpzRxo0b9dJLLyk/P1+1a9f2cIXhKzo6Wp06ddLevXu9Xsol4TMWRerWratu3bpp7dq1AfNr165Vz549PVoVajrnnEaPHq233npL//nPf9S6dWuvl1StOOeUn5/v9TLCys0336xdu3Zpx44dxSMpKUn333+/duzYQai4BPn5+dqzZ4+aNWvm9VIuCa9YnGf8+PF64IEHlJSUpOTkZL388ss6dOiQHnvsMa+XFnaOHTumL7/8svj2/v37tWPHDsXFxalFixYeriy8jBo1SosXL9bKlSsVExNT/IpabGysoqKiPF5deHnqqafUv39/JSYmKi8vT0uWLNH69eu1evVqr5cWVmJiYkp9xic6OlqNGjXisz+VNGHCBA0cOFAtWrTQ0aNHNX36dOXm5iotLc3rpV0SgsV5fv3rX+u7777TM888o8OHD6tjx45atWqVWrZs6fXSwk5GRob69OlTfPvc+4VpaWl6/fXXPVpV+Dm39bl3794B8wsXLtTQoUNDv6Awlp2drQceeECHDx9WbGysOnfurNWrV6tv375eLw011P/+9z/dd999ysnJ0eWXX64ePXpo69atYf8zh99jAQAAzPAZCwAAYIZgAQAAzBAsAACAGYIFAAAwQ7AAAABmCBYAAMAMwQIAAJghWAAAADMECwAAYIZgAQAAzBAsAACAGYIFAAAw8/9/dcIOEFUu6QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def move_state(state, direction):\n",
    "    dir_right = direction == 1  # true where there is +1, else False\n",
    "    dir_left  = direction == -1  # true where there is -1, else False\n",
    "    dir_up    = direction == 2  # true where there is -1, else False\n",
    "    dir_down  = direction == -2  # true where there is +1, else False\n",
    "\n",
    "    state_right = state*dir_right\n",
    "    state_left  = state*dir_left\n",
    "    state_up    = state*dir_up\n",
    "    state_down  = state*dir_down\n",
    "\n",
    "    state_right_new = torch.roll(state_right, shifts=1, dims=1)    # shift state_right right by 1\n",
    "    state_left_new = torch.roll(state_left, shifts=-1, dims=1)    # shift state_right left by 1\n",
    "    state_up_new = torch.roll(state_up, shifts=-1, dims=0)    # shift state_up up by 1\n",
    "    state_down_new = torch.roll(state_down, shifts=1, dims=0)    # shift state_down down by 1\n",
    "\n",
    "    new_state = state_left_new + state_right_new + state_up_new + state_down_new\n",
    "\n",
    "    return new_state\n",
    "\n",
    "state = torch.tensor([[0, 0, 0, 0, 0, 1],\n",
    "                      [0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0]], dtype=torch.bool)\n",
    "\n",
    "connections  = torch.tensor([[0, 0, 0, -2, -1, -1],\n",
    "                             [2, 0, 0, -2, 0, 2],\n",
    "                             [0, 0, 0, 1, 1, 2]], dtype=torch.int)\n",
    "\n",
    "states = []\n",
    "for time_step in range(100):\n",
    "    state_nump = state.numpy()\n",
    "    plt.xticks(ticks=range(state_nump.shape[1]), labels=range(state_nump.shape[1]))    # Remove axis ticks\n",
    "    plt.yticks(ticks=range(state_nump.shape[0]), labels=range(state_nump.shape[0]))\n",
    "    plt.imshow(state_nump, cmap='gray', aspect='equal')\n",
    "\n",
    "    # put the value of each pixel in the image\n",
    "    for i in range(state_nump.shape[0]): \n",
    "        for j in range(state_nump.shape[1]):  plt.text(j, i, f'{state_nump[i, j]:.2f}', color='red', ha='center', va='center')\n",
    "\n",
    "    plt.title(f\"time step: {time_step}\")\n",
    "    # plt.show()\n",
    "\n",
    "    clear_output(wait=True) # Clear the previous output\n",
    "    display(plt.gcf())      # Display the current figure\n",
    "    plt.close()             # Close the figure to prevent display overlap\n",
    "    time.sleep(0.1)         # Pause for a short period of time to create an animation effect\n",
    "\n",
    "    state = move_state(state, connections)\n",
    "    states.append(state)\n",
    "\n",
    "# plot_1D_2D_brain(connections, 'connections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in multi-DIR pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m display(plt\u001b[38;5;241m.\u001b[39mgcf())      \u001b[38;5;66;03m# Display the current figure\u001b[39;00m\n\u001b[1;32m     80\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()             \u001b[38;5;66;03m# Close the figure to prevent display overlap\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2.0\u001b[39m)         \u001b[38;5;66;03m# Pause for a short period of time to create an animation effect\u001b[39;00m\n\u001b[1;32m     83\u001b[0m state \u001b[38;5;241m=\u001b[39m move_state(state, connections_and_strength, threshold, p_spontaneous\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     84\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def move_state(state, connections_and_strength, threshold, p_spontaneous):\n",
    "    # apply thresholding\n",
    "    new_state = torch.zeros_like(state, dtype=state.dtype)\n",
    "    indices = state >= threshold\n",
    "    new_state[indices] = state[indices] - threshold[indices]\n",
    "    new_state[new_state != 0] = 1.0\n",
    "\n",
    "    state_right = new_state*connections_and_strength[0]\n",
    "    state_left  = new_state*connections_and_strength[1]\n",
    "    state_up    = new_state*connections_and_strength[2]\n",
    "    state_down  = new_state*connections_and_strength[3]\n",
    "\n",
    "    state_right_new = torch.roll(state_right, shifts=1, dims=1)    # shift state_right right by 1\n",
    "    state_left_new = torch.roll(state_left, shifts=-1, dims=1)    # shift state_left left by 1\n",
    "    state_up_new = torch.roll(state_up, shifts=-1, dims=0)    # shift state_up up by 1\n",
    "    state_down_new = torch.roll(state_down, shifts=1, dims=0)    # shift state_down down by 1\n",
    "\n",
    "    state_right_new[:, 0] = 0.0\n",
    "    state_left_new[:, -1] = 0.0\n",
    "    state_up_new[-1, :] = 0.0\n",
    "    state_down_new[0, :] = 0.0\n",
    "\n",
    "    new_state = state_left_new + state_right_new + state_up_new + state_down_new\n",
    "\n",
    "    # spontaneous firing\n",
    "    spontaneous_firing = torch.rand_like(new_state, dtype=torch.float32) < p_spontaneous\n",
    "    new_state[spontaneous_firing] += strength[spontaneous_firing]\n",
    "\n",
    "    return new_state\n",
    "\n",
    "state = torch.tensor([[0.0, 0.0, 0.0, 0.0, 0.0, 10.0],\n",
    "                      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], dtype=torch.uint8)\n",
    "\n",
    "threshold = torch.tensor([[9.0, 9.0, 4.0, 4.0, 4.0, 4.0],  # minumum threshold is 1.0!!! this ensures the sumultanious pulsing works!\n",
    "                          [1.0, 1.0, 1.0, 4.0, 1.0, 4.0],\n",
    "                          [1.0, 1.0, 1.0, 4.0, 4.0, 4.0]], dtype=torch.uint8)\n",
    "\n",
    "connections_and_strength = [torch.tensor([[0, 0, 0, 0, 0, 0],# right # existance = connection, value = weight \n",
    "                                          [0, 0, 0, 0, 0, 0],\n",
    "                                          [0, 0, 0, 5, 5, 0]], dtype=torch.uint8),\n",
    "                            torch.tensor([[1, 1, 1, 1, 5, 5],# left\n",
    "                                          [0, 0, 0, 0, 0, 0],\n",
    "                                          [0, 0, 0, 0, 0, 0]], dtype=torch.uint8),\n",
    "                            torch.tensor([[0, 0, 0, 0, 0, 0],# up\n",
    "                                          [0, 0, 0, 0, 0, 5],\n",
    "                                          [0, 0, 0, 0, 0, 5]], dtype=torch.uint8),\n",
    "                            torch.tensor([[0, 0, 0, 5, 0, 0],# down\n",
    "                                          [0, 0, 0, 5, 0, 0],\n",
    "                                          [0, 0, 0, 0, 0, 0]], dtype=torch.uint8),]\n",
    "\n",
    "# make sure to add a tensor of ones to the list of tensors\n",
    "connections_and_strength.append(torch.ones_like(connections_and_strength[0]))\n",
    "stacked_tensors = torch.stack(connections_and_strength)\n",
    "strength = torch.max(stacked_tensors, dim=0)[0]\n",
    "\n",
    "states = []\n",
    "for time_step in range(100):\n",
    "    state_nump = state.numpy()\n",
    "    plt.xticks(ticks=range(state_nump.shape[1]), labels=range(state_nump.shape[1]))    # Remove axis ticks\n",
    "    plt.yticks(ticks=range(state_nump.shape[0]), labels=range(state_nump.shape[0]))\n",
    "    plt.imshow(state_nump, cmap='gray', aspect='equal')\n",
    "\n",
    "    # put the value of each pixel in the image\n",
    "    for i in range(state_nump.shape[0]): \n",
    "        for j in range(state_nump.shape[1]):  plt.text(j, i, f'{state_nump[i, j]:.2f}', color='red', ha='center', va='center')\n",
    "\n",
    "    plt.title(f\"time step: {time_step}\")\n",
    "\n",
    "    clear_output(wait=True) # Clear the previous output\n",
    "    display(plt.gcf())      # Display the current figure\n",
    "    plt.close()             # Close the figure to prevent display overlap\n",
    "    time.sleep(2.0)         # Pause for a short period of time to create an animation effect\n",
    "\n",
    "    state = move_state(state, connections_and_strength, threshold, p_spontaneous=0.1)\n",
    "    states.append(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a large one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_state\n\u001b[1;32m     39\u001b[0m shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     43\u001b[0m threshold \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     45\u001b[0m connections_and_strength \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     46\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     47\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     48\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)]\n",
      "Cell \u001b[0;32mIn[60], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_state\n\u001b[1;32m     39\u001b[0m shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     43\u001b[0m threshold \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     45\u001b[0m connections_and_strength \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     46\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     47\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8),\n\u001b[1;32m     48\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(shape), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)]\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/ros_env/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ros_env/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def move_state(state, connections_and_strength, threshold, p_spontaneous):\n",
    "    # apply thresholding\n",
    "    new_state = torch.zeros_like(state, dtype=state.dtype)\n",
    "    indices = state >= threshold\n",
    "    new_state[indices] = state[indices] - threshold[indices]\n",
    "    new_state[new_state != 0] = 1.0\n",
    "\n",
    "    state_right = new_state*connections_and_strength[0]\n",
    "    state_left  = new_state*connections_and_strength[1]\n",
    "    state_up    = new_state*connections_and_strength[2]\n",
    "    state_down  = new_state*connections_and_strength[3]\n",
    "\n",
    "    state_right_new = torch.roll(state_right, shifts=1, dims=1)    # shift state_right right by 1\n",
    "    state_left_new = torch.roll(state_left, shifts=-1, dims=1)    # shift state_left left by 1\n",
    "    state_up_new = torch.roll(state_up, shifts=-1, dims=0)    # shift state_up up by 1\n",
    "    state_down_new = torch.roll(state_down, shifts=1, dims=0)    # shift state_down down by 1\n",
    "\n",
    "    state_right_new[:, 0] = 0.0\n",
    "    state_left_new[:, -1] = 0.0\n",
    "    state_up_new[-1, :] = 0.0\n",
    "    state_down_new[0, :] = 0.0\n",
    "\n",
    "    new_state = state_left_new + state_right_new + state_up_new + state_down_new\n",
    "\n",
    "    # spontaneous firing\n",
    "    spontaneous_firing = torch.rand_like(new_state, dtype=torch.float32) < p_spontaneous\n",
    "    new_state[spontaneous_firing] += strength[spontaneous_firing]\n",
    "\n",
    "    return new_state\n",
    "\n",
    "shape = [100, 100]\n",
    "\n",
    "state = torch.randint(0, 10, tuple(shape), dtype=torch.uint8)\n",
    "\n",
    "threshold = torch.randint(0, 10, tuple(shape), dtype=torch.uint8)\n",
    "\n",
    "connections_and_strength = [torch.randint(0, 10, tuple(shape), dtype=torch.uint8),\n",
    "                            torch.randint(0, 10, tuple(shape), dtype=torch.uint8),\n",
    "                            torch.randint(0, 10, tuple(shape), dtype=torch.uint8),\n",
    "                            torch.randint(0, 10, tuple(shape), dtype=torch.uint8)]\n",
    "\n",
    "# make sure to add a tensor of ones to the list of tensors\n",
    "connections_and_strength.append(torch.ones_like(connections_and_strength[0]))\n",
    "stacked_tensors = torch.stack(connections_and_strength)\n",
    "strength = torch.max(stacked_tensors, dim=0)[0]\n",
    "\n",
    "states = []\n",
    "for time_step in range(100):\n",
    "\n",
    "    state = move_state(state, connections_and_strength, threshold, p_spontaneous=0.1)\n",
    "    states.append(state)\n",
    "\n",
    "# 24FPS for a 1000x1000 grid (1M neurons)\n",
    "# 500 fps at 100x100 grid    (10k neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to start learning!\n",
    "We can learn from:\n",
    "- Spike Time Dependant Plasticity (Hebbian learning), where the relative timing of spikes between neurons determines whether synapses are strengthened or weakened. For example, if a presynaptic neuron fires just before a postsynaptic neuron, the connection strengthens, but if the timing is reversed, the connection weakens.\n",
    "- Using rewards to learn.\n",
    "- can the system be differential and use backprop? -> I think so, although maybe the randomised spontanious firing get in the way here? can we sample and use the reparameterisation trick?\n",
    "- Hormones and chemistry?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wmandil/miniforge3/envs/ros_env/lib/python3.11/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Sample random action\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# action = env.action_space.sample()\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Step in the environment\u001b[39;00m\n\u001b[1;32m    115\u001b[0m observation, reward, done, info, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[65], line 36\u001b[0m, in \u001b[0;36mBrain.step\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# scale the input to the range of the state with min max normalization (input is -1, 1)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43minput\u001b[39;49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m128\u001b[39m \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m    \u001b[38;5;66;03m# add the input to the first layer of the state\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m move_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections_and_strength, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, p_spontaneous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_spontaneous)\n",
      "Cell \u001b[0;32mIn[65], line 36\u001b[0m, in \u001b[0;36mBrain.step\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# scale the input to the range of the state with min max normalization (input is -1, 1)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43minput\u001b[39;49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m128\u001b[39m \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m    \u001b[38;5;66;03m# add the input to the first layer of the state\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m move_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections_and_strength, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, p_spontaneous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_spontaneous)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/ros_env/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ros_env/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, shape):\n",
    "        self.min_ouptut = 0\n",
    "        self.max_output = 256\n",
    "\n",
    "        self.p_spontaneous = 0.1\n",
    "\n",
    "        self.state = torch.randint(0, 10, tuple(shape), dtype=torch.uint8)\n",
    "\n",
    "        self.threshold = torch.randint(0, 10, tuple(shape), dtype=torch.uint8)\n",
    "\n",
    "        self.connections_and_strength = [torch.randint(0, 10, tuple(shape), dtype=torch.uint8),\n",
    "                                         torch.randint(0, 10, tuple(shape), dtype=torch.uint8),\n",
    "                                         torch.randint(0, 10, tuple(shape), dtype=torch.uint8),\n",
    "                                         torch.randint(0, 10, tuple(shape), dtype=torch.uint8)]\n",
    "\n",
    "        # make sure to add a tensor of ones to the list of tensors\n",
    "        connections_and_strength.append(torch.ones_like(connections_and_strength[0]))\n",
    "        stacked_tensors = torch.stack(connections_and_strength)\n",
    "        self.strength = torch.max(stacked_tensors, dim=0)[0]\n",
    "\n",
    "    def step(self, input):\n",
    "        # scale the input to the range of the state with min max normalization (input is -1, 1)\n",
    "        input = (input[0] + 1) * 128\n",
    "\n",
    "        self.state[:, 0] = torch.tensor(input)    # add the input to the first layer of the state\n",
    "\n",
    "        self.state = move_state(self.state, self.connections_and_strength, self.threshold, p_spontaneous=self.p_spontaneous)\n",
    "        output = self.state[-1]    # the last layer of the state is the output\n",
    "\n",
    "        output = (output - self.min_ouptut) / (self.max_output - self.min_ouptut)        # scale the output to the range of the action space with min max normalization\n",
    "\n",
    "        return output\n",
    "\n",
    "    def move_state(self, state, connections_and_strength, threshold, p_spontaneous):\n",
    "        # apply thresholding\n",
    "        new_state = torch.zeros_like(state, dtype=state.dtype)\n",
    "        indices = state >= threshold\n",
    "        new_state[indices] = state[indices] - threshold[indices]\n",
    "        new_state[new_state != 0] = 1.0\n",
    "\n",
    "        state_right = new_state*connections_and_strength[0]\n",
    "        state_left  = new_state*connections_and_strength[1]\n",
    "        state_up    = new_state*connections_and_strength[2]\n",
    "        state_down  = new_state*connections_and_strength[3]\n",
    "\n",
    "        state_right_new = torch.roll(state_right, shifts=1, dims=1)    # shift state_right right by 1\n",
    "        state_left_new = torch.roll(state_left, shifts=-1, dims=1)    # shift state_left left by 1\n",
    "        state_up_new = torch.roll(state_up, shifts=-1, dims=0)    # shift state_up up by 1\n",
    "        state_down_new = torch.roll(state_down, shifts=1, dims=0)    # shift state_down down by 1\n",
    "\n",
    "        state_right_new[:, 0] = 0.0\n",
    "        state_left_new[:, -1] = 0.0\n",
    "        state_up_new[-1, :] = 0.0\n",
    "        state_down_new[0, :] = 0.0\n",
    "\n",
    "        new_state = state_left_new + state_right_new + state_up_new + state_down_new\n",
    "\n",
    "        # spontaneous firing\n",
    "        if p_spontaneous > 0:\n",
    "            spontaneous_firing = torch.rand_like(new_state, dtype=torch.float32) < p_spontaneous\n",
    "            new_state[spontaneous_firing] += self.strength[spontaneous_firing]\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def print_state(self, state, time_step):\n",
    "        state_nump = state.numpy()\n",
    "        plt.xticks(ticks=range(state_nump.shape[1]), labels=range(state_nump.shape[1]))    # Remove axis ticks\n",
    "        plt.yticks(ticks=range(state_nump.shape[0]), labels=range(state_nump.shape[0]))\n",
    "        plt.imshow(state_nump, cmap='gray', aspect='equal')\n",
    "\n",
    "        # put the value of each pixel in the image\n",
    "        for i in range(state_nump.shape[0]): \n",
    "            for j in range(state_nump.shape[1]):  plt.text(j, i, f'{state_nump[i, j]:.2f}', color='red', ha='center', va='center')\n",
    "\n",
    "        plt.title(f\"time step: {time_step}\")\n",
    "\n",
    "        clear_output(wait=True) # Clear the previous output\n",
    "        display(plt.gcf())      # Display the current figure\n",
    "        plt.close()             # Close the figure to prevent display overlap\n",
    "        time.sleep(0.1)         # Pause for a short period of time to create an animation effect\n",
    "\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "\n",
    "# Create the brain\n",
    "brain = Brain(shape=[4, 10])\n",
    "\n",
    "for _ in range(1000):\n",
    "    # Render the environment\n",
    "    env.render()\n",
    "\n",
    "    # Sample random action\n",
    "    action = brain.step(observation)\n",
    "\n",
    "    # action = env.action_space.sample()\n",
    "\n",
    "    # Step in the environment\n",
    "    observation, reward, done, info, _ = env.step(action)\n",
    "\n",
    "    brain.print_state(brain.state, time_step)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_1D_brain(data, title):\n",
    "    # plot as an image\n",
    "    if data.dim() == 1:\n",
    "        # Reshape the tensor to a 2D array with shape (1, len(data))\n",
    "        data = data.unsqueeze(0)\n",
    "\n",
    "    # convert to numpy\n",
    "    data = data.numpy()\n",
    "    # Remove axis ticks\n",
    "    plt.xticks(ticks=range(data.shape[1]), labels=range(data.shape[1]))\n",
    "    plt.yticks(ticks=range(data.shape[0]), labels=range(data.shape[0]))\n",
    "\n",
    "    plt.imshow(data, cmap='gray', aspect='equal')\n",
    "\n",
    "    # put the value of each pixel in the image\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            plt.text(j, i, f'{data[i, j]:.2f}', color='red', ha='center', va='center')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def move_state(state, direction):\n",
    "    state = state.bool()\n",
    "    direction = direction.int()\n",
    "    print(\"state\", state)\n",
    "    print(\"direction\", direction)\n",
    "\n",
    "    # Get positions indices\n",
    "    positions = torch.arange(state.size(0), device=state.device)\n",
    "    print(\"positions = torch.arange(state.size(0), device=state.device)\")\n",
    "    print(positions)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Compute new positions\n",
    "    new_positions = positions + direction\n",
    "    print(\"new_positions = positions + direction\")\n",
    "    print(new_positions)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Clamp new positions to stay within valid range\n",
    "    new_positions = torch.clamp(new_positions, 0, state.size(0) - 1)\n",
    "    print(\"new_positions = torch.clamp(new_positions, 0, state.size(0) - 1)\")\n",
    "    print(new_positions)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Create a mask for positions where state is True (1)\n",
    "    mask = state\n",
    "    print(\"mask = state\")\n",
    "    print(mask)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Initialize new state with zeros\n",
    "    new_state = torch.zeros_like(state, dtype=torch.bool)\n",
    "    print(\"new_state = torch.zeros_like(state, dtype=torch.bool)\")\n",
    "    print(new_state)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    # Move '1's to new positions\n",
    "    new_state.index_put_((new_positions[mask],), torch.tensor(True, device=state.device), accumulate=True)\n",
    "    print(\"new_state.index_put_((new_positions[mask],), torch.tensor(True, device=state.device), accumulate=True)\")\n",
    "    print(new_state)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return new_state\n",
    "\n",
    "\n",
    "state = torch.tensor([1, 0, 0, 1, 0, 1], dtype=torch.bool)\n",
    "connections  = torch.tensor([1, 1, 1, -1, 1, -1], dtype=torch.int)\n",
    "plot_1D_brain(state, '1D Brain')\n",
    "plot_1D_brain(connections, 'connections')\n",
    "\n",
    "\n",
    "new_state = move_state(state, connections)\n",
    "plot_1D_brain(new_state, '1D Brain')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
